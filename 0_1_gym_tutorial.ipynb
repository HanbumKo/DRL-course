{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0_1_gym_tutorial.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HanbumKo/DRL-course/blob/main/0_1_gym_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzC5MOKcsFFN"
      },
      "source": [
        "# Gym Tutorial\n",
        "Gym은 Open AI에서 개발한 강화학습 환경(Environment)을 정의해주는 프레임워크라 할 수 있겠습니다.\n",
        "\n",
        "![rl](https://cdn-media-1.freecodecamp.org/images/1*vz3AN1mBUR2cr_jEG8s7Mg.png)\n",
        "\n",
        "(an image from https://www.freecodecamp.org/news/a-brief-introduction-to-reinforcement-learning-7799af5840db/)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "위 그림과 같이 Agent와 Environment가 있는 강화학습의 시나리오에서, 우리가 Agent의 입장이 되어 Gym으로 정의된 환경으로부터 State와 Reward를 받을 수 있게됩니다.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cdDTJFjus0q"
      },
      "source": [
        "이 튜토리얼에서는 Gym을 간단한 예제와 함께 살펴봅니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLo13UPKr435"
      },
      "source": [
        "import gym\n",
        "\n",
        "print(\"import complete\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmyEpxkvvSLI"
      },
      "source": [
        "Google Colab에서는 몇 가지 이미 설치된 라이브러리들이 제공되는데, 그 중 gym도 기본으로 제공됩니다. 자신의 파이썬 환경에서 사용시는\n",
        "\n",
        "\n",
        "```\n",
        "pip install gym\n",
        "```\n",
        "\n",
        "으로 간단히 설치할 수 있습니다.\n",
        "\n",
        "혹은 직접 소스를 다운받고 다양한 환경을 한번에 설치할 수도 있습니다.\n",
        "\n",
        "\n",
        "```\n",
        "git clone https://github.com/openai/gym\n",
        "cd gym\n",
        "pip install -e .[all]\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaXK0td_wYj8"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Example code\n",
        "\n",
        "다음으로 OpenAI gym 공식 홈페이지에서 제공하는 문서의 예제코드를 살펴보도록 하겠습니다.\n",
        "\n",
        "참고) 이 문서에서는 state와 observation을 구별하지 않고 같은 의미로 사용하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsd5car4sXuJ"
      },
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "env.reset()\n",
        "for _ in range(1000):\n",
        "    # env.render()\n",
        "    env.step(env.action_space.sample()) # take a random action\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9pkoV0FylSg"
      },
      "source": [
        "위 코드는 gym으로 정의된 환경을 만들고, 실행하는 최소한의 코드가 되겠습니다.\n",
        "\n",
        "gym.make('환경이름')을 통해 환경을 만들 수 있고,\n",
        "\n",
        "env.step(action)을 통해 환경에 action을 내릴 수 있습니다.\n",
        "\n",
        "env.reset()은 환경을 리셋해주는 역할을 하면서 첫번째 state(or observation)를 반환하여 줍니다.\n",
        "\n",
        "env.render()는 환경이 잘 동작하는지 확인하기 위해 GUI를 통해 보여주는 함수입니다. Colab 혹은 remote server 환경에서는 동작하지 않습니다. (로컬에서 사용 시 확인해보시길 추천드립니다.)\n",
        "\n",
        "위 코드에서는 random action을 주고있습니다.\n",
        "\n",
        "env.close()는 만약 환경이 메모리에 접근하거나 쓰레드를 사용하는 등 정리해주지 않으면 실행에 영향을 줄 수 있는 부분을 정리해주는 함수입니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZKGhff70IFl"
      },
      "source": [
        "다음으로 조금 더 복잡한 코드를 살펴보도록 하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saTpllClw29g"
      },
      "source": [
        "for i_episode in range(20):\n",
        "    state = env.reset()\n",
        "    for t in range(100):\n",
        "        # env.render()\n",
        "        print(state)\n",
        "        action = env.action_space.sample()\n",
        "        state, reward, done, info = env.step(action)\n",
        "        if done:\n",
        "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
        "            break\n",
        "\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWV9FWrL1VIt"
      },
      "source": [
        "위 코드에서는 총 20번의 에피소드를 실행하게 됩니다. -> for i_episode in range(20):\n",
        "\n",
        "env.reset()를 에피소드의 처음에 실행하여 처음 state를 받아옵니다. -> state = env.reset()\n",
        "\n",
        "이제 maximum 100 timestep을 돌면서 -> for t in range(100):\n",
        "\n",
        "현재 timestep에서의 state를 보고 Agent의 action을 계산합니다. 위 코드에서는 random action을 받아옵니다. -> action = env.action_space.sample()\n",
        "\n",
        "env.step(action)은 총 4가지 object를 반환하는데 state는 action이 실행되고 난 이후의 다음 state, reward는 action 실행에 대한 reward, 에피소드가 종료되었는지 나타내주는 done, 기타 정보를 담은 info를 반환하여 줍니다. -> state, reward, done, info = env.step(action)\n",
        "\n",
        "if done:을 추가하여 에피소드가 종료되면 몇번째 timestep에서 에피소드가 종료되었는지를 출력하여주고 break를 통해 다음 에피소드를 실행하게 됩니다.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiHoK-rj3-tG"
      },
      "source": [
        "env.render()함수 사용 시 아래와 같은 화면을 볼 수 있습니다.\n",
        "\n",
        "[cartpole](http://s3-us-west-2.amazonaws.com/rl-gym-doc/cartpole-yes-reset.mp4)\n",
        "\n",
        "현재 사용하고 있는 CartPole 환경은 아래 네모난 박스를 좌우로 이동(action)하여 막대가 떨어지지 않도록 하는것이 목표입니다. 막대가 아래로 떨어지면 에피소드가 끝나게됩니다.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O23Gd7kb6m1n"
      },
      "source": [
        "# Spaces\n",
        "\n",
        "gym에서 action과 state를 나타내는데에 Space라는 개념을 만들어서 사용합니다. state와 action 모두 discrete 혹은 continuous가 될 수 있는데, 이것을 정의하고 환경을 사용하는 유저에게 정보를 전달함과 동시에 위 코드에서 사용한 env.action_space.sample()와 같이 몇 가지 함수를 사용할 수 있도록 도와줍니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXL7zkrv1C3z"
      },
      "source": [
        "print(env.action_space)\n",
        "#> Discrete(2)\n",
        "print(env.observation_space)\n",
        "#> Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niryBIZ-90uf"
      },
      "source": [
        "환경에 정의된 action_space와 observation_space를 위와 같이 확인할 수 있는데,\n",
        "\n",
        "action_space에서는 두 가지의 discrete action,\n",
        "\n",
        "observation_space에서는 shape가 (4,)인 continuous 값을 사용하는 것을 확인할 수 있습니다.\n",
        "\n",
        "Box가 continuous를 뜻하며 minimum 값, maximum 값, shape, 데이터타입을 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rqRdhjG9rzU"
      },
      "source": [
        "print(env.observation_space.high)\n",
        "#> [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n",
        "print(env.observation_space.low)\n",
        "#> [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9u6P6ifv-3xf"
      },
      "source": [
        "Box(continuous)사용시에는 .high와 .low를 사용하여 각 값들의 maximum 값과 minimum 값들을 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4pSgIM599DA"
      },
      "source": [
        "from gym import spaces\n",
        "space = spaces.Discrete(8) # Set with 8 elements {0, 1, 2, ..., 7}\n",
        "x = space.sample()\n",
        "assert space.contains(x)\n",
        "assert space.n == 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxcSQEmo_TKg"
      },
      "source": [
        "gym라이브러리에서 spaces만 import 하여 확인해 볼 수 있습니다.\n",
        "\n",
        "Discrete(액션 개수)를 정의하고 .sample을 해주게 된다면 {0, 1, 2, ..., 7} 값 중에서 하나가 랜덤하게 선택되는 것을 확인할 수 있습니다.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rj3V9Lz6_2nZ"
      },
      "source": [
        "from gym import envs\n",
        "print(envs.registry.all())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDkM-6oxAJG4"
      },
      "source": [
        "위 코드에서는 현재 사용하고 있는 gym에서 사용할 수 있는 환경들을 볼 수 있습니다."
      ]
    }
  ]
}