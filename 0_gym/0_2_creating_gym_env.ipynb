{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0_2_creating_gym_env.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPTAxUcsyD0XVwV876lVKRn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HanbumKo/DRL-course/blob/main/0_gym/0_2_creating_gym_env.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3cr-LGhK9Um"
      },
      "source": [
        "# Creating Gym Env\n",
        "\n",
        "이번 노트북에서는 gym 환경을 직접 제작하는 방법을 살펴보겠습니다.\n",
        "\n",
        "환경을 제작하여서 python package 형태로 제공할 수도 있지만 jupyter notebook상에서는 gym 환경 클래스를 직접 정의하고 불러오도록 하겠습니다.\n",
        "\n",
        "package 형태로 제작하는 방법은 [이곳](https://github.com/openai/gym/blob/master/docs/creating-environments.md)을 참고하시면 되겠습니다.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWfnZTKBK4s7"
      },
      "source": [
        "import gym\n",
        "\n",
        "\n",
        "class SimpleEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        # Some initialization for Env\n",
        "        pass\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, done, info = 0, 0, True, {}\n",
        "        return obs, reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        obs = 0\n",
        "        return obs\n",
        "\n",
        "    def render(self):\n",
        "        # Code for visualization\n",
        "        pass\n",
        "\n",
        "    def close(self):\n",
        "        # Clear env\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gw2sSBkOFBo"
      },
      "source": [
        "gym 환경을 구현하려면 gym.Env를 상속받는 클래스를 구현해야합니다.\n",
        "\n",
        "위와같이 gym.Env를 상속받고 총 5개 함수를 쓰임새에 맞게 구현해주면 되겠습니다.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hweg3zDtOhbl"
      },
      "source": [
        "# GridWorld\n",
        "\n",
        "![gridworld](https://github.com/HanbumKo/DRL-course/blob/main/0_gym/images/gridworld.png?raw=true)\n",
        "\n",
        "위 그림과 같이 간단한 2x4 GridWorld 에서의 환경을 구현해보도록 하겠습니다. Agent(스마일)가 별을 찾도록 하는것을 목표로 하는 환경으로 구성해보겠습니다.\n",
        "\n",
        "\n",
        "observation은 Agent위에 아무것도 없으면 0, 금지표시가 있으면 1, 별이 있으면 2로 표현하도록 하겠습니다.\n",
        "\n",
        "action은 (위, 아래, 왼쪽, 오른쪽)로의 움직임을 각각 (0, 1, 2, 3)으로 사용하도록 하겠습니다.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvh4G2ezZJzk"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "from gym import spaces\n",
        "\n",
        "\n",
        "class GridEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        self.world_shape = [2, 4]\n",
        "        self.agent_pos = [1, 0]\n",
        "        self.restrict_pos = [0, 1]\n",
        "        self.star_pos = [0, 3]\n",
        "        self.action_space = spaces.Discrete(4) # (위, 아래, 왼쪽, 오른쪽) 4가지 action\n",
        "        self.obs_space = spaces.Discrete(3) # (빈 곳, 금지표시, 별) 3가지 observation\n",
        "\n",
        "    def step(self, action):\n",
        "        if action == 0:\n",
        "            self.agent_pos[0] -= 1\n",
        "        elif action == 1:\n",
        "            self.agent_pos[0] += 1\n",
        "        elif action == 2:\n",
        "            self.agent_pos[1] -= 1\n",
        "        elif action == 3:\n",
        "            self.agent_pos[1] += 1\n",
        "        else:\n",
        "            raise Exception(\"the action is not defined\")\n",
        "    \n",
        "        info = {}\n",
        "\n",
        "        return self._get_obs(), self._get_reward(), self._is_done(), info\n",
        "\n",
        "    def reset(self):\n",
        "        self.world = np.zeros(self.world_shape)\n",
        "        self.world[0, 1] = 1 # 금지표시\n",
        "        self.world[0, 3] = 2 # 별\n",
        "        self.agent_pos = [1, 0]\n",
        "\n",
        "        return self._get_obs()\n",
        "\n",
        "    def render(self):\n",
        "        self.world[self.agent_pos[0], self.agent_pos[1]] = -1\n",
        "        print(self.world)\n",
        "        self.world[self.agent_pos[0], self.agent_pos[1]] = 0\n",
        "\n",
        "    def close(self):\n",
        "        pass\n",
        "\n",
        "    def _get_obs(self):\n",
        "        if self.agent_pos == [1, 1]:\n",
        "            return 1 # 금지표시 아래\n",
        "        elif self.agent_pos == [1, 3]:\n",
        "            return 2 # 별 아래\n",
        "        else:\n",
        "            return 0 # 빈 곳\n",
        "    \n",
        "    def _get_reward(self):\n",
        "        # 금지표시\n",
        "        if self.agent_pos == self.restrict_pos:\n",
        "            return -1\n",
        "        # 별 cases\n",
        "        elif self.agent_pos == self.star_pos:\n",
        "            return 1\n",
        "        # 빈곳\n",
        "        else:\n",
        "            return 0\n",
        "    \n",
        "    def _is_done(self):\n",
        "        # 맵 밖으로 나갔을 때\n",
        "        if self.agent_pos[0] < 0 or self.agent_pos[0] > 1:\n",
        "            return True\n",
        "        if self.agent_pos[1] < 0 or self.agent_pos[1] > 3:\n",
        "            return True\n",
        "        # 금지표시\n",
        "        if self.agent_pos == self.restrict_pos:\n",
        "            return True\n",
        "        # 별\n",
        "        if self.agent_pos == self.star_pos:\n",
        "            return True\n",
        "        # 나머지\n",
        "        else:\n",
        "            return False\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPaefw51gNuI"
      },
      "source": [
        "위에서 설명한 GridWorld 환경을 코드로 구현한 결과입니다. action space를 4개의 discrete action으로 정의해 주었고 observation space는 3개의 discrete observation으로 설정해 주었습니다. 또한 \n",
        "\n",
        "\n",
        "```\n",
        "self.action_space = spaces.Discrete(4) # (위, 아래, 왼쪽, 오른쪽) 4가지 action\n",
        "self.obs_space = spaces.Discrete(3) # (빈 곳, 금지표시, 별) 3가지 observation\n",
        "\n",
        "```\n",
        "\n",
        "step() 함수에서는 들어온 action에 따라 agent_pos를 조절 후 그에 맞게 다음 observation, reward, done을 반환 하도록 하였습니다.\n",
        "\n",
        "render()에서는 간단히 numpy array를 print 해서 visualization을 해주는데, python의 GUI 라이브러리를 사용하여 표현해 줄 수도 있습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oB23bXfPa3jQ"
      },
      "source": [
        "env = GridEnv()\n",
        "\n",
        "for i_episode in range(20):\n",
        "    obs = env.reset()\n",
        "    for t in range(100):\n",
        "        env.render()\n",
        "        print(obs)\n",
        "        # action = env.action_space.sample() # random action\n",
        "        action = 3 # Move right\n",
        "        obs, reward, done, info = env.step(action)\n",
        "        if done:\n",
        "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
        "            break\n",
        "\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVe7l-z9h6va"
      },
      "source": [
        "이제 위에서 구현한 GridEnv를 사용해볼 차례입니다. action을 무조건 오른쪽(3)으로 주고 이에 따라 observation이 바뀌는 것을 확인해 볼 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZST3OO6liZRQ"
      },
      "source": [
        "# Homework\n",
        "\n",
        "새로운 GridWorld 환경을 만들어 16 x 16 크기로 구성한 다음 다음과 같이 구현해주세요.\n",
        "\n",
        "\n",
        "\n",
        "1.   reset()시 agent의 위치, 별의 위치, 금지표시의 위치를 random하게 생성\n",
        "2.   observation은 12개의 discrete space로 구성하여 1-4는 (위, 아래, 왼쪽, 오른쪽)으로 벽이 얼마나 떨어져 있는지, 5-8은 (위, 아래, 왼쪽, 오른쪽)으로 금지표시가 얼마나 떨어져 있는지, 9-12는 (위, 아래, 왼쪽, 오른쪽)으로 별 표시가 얼마나 떨어져 있는지 반환 (spaces.Tuple과 spaces.Discrete를 동시에 사용하여야 합니다. 사용방법은 [이곳](https://github.com/openai/gym-soccer/blob/master/gym_soccer/envs/soccer_env.py#L29-L34)을 참고해주세요.)\n",
        "3.   reward는 예제와 같이 구성\n",
        "4.   done도 예제와 같이 구성\n",
        "\n",
        "## bonus\n",
        "\n",
        "python의 GUI 라이브러리를 찾고 render function 사용 시 GUI로 볼 수 있도록 구성\n"
      ]
    }
  ]
}